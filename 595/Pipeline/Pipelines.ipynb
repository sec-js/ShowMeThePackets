{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Pipelines\n",
    "\n",
    "## Overview\n",
    "\n",
    "SKLearn Pipelines allow you to simplify the final implementation of a model from data ingestion to inference into a neat package that is easily deployed. The first few cells largely replicate the 95% solution for BackBlaze and build a pipeline. The code for saving the pipeline is commented out. \n",
    "\n",
    "The more interesting code is at the end where we load a saved pipeline and use it with test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Backblaze data\n",
    "import numpy as np\n",
    "import polars as pl\n",
    "import glob\n",
    "\n",
    "# Let's bring our accuracy function\n",
    "def accuracy(y_hat, y):\n",
    "    accuracy = 0\n",
    "    correct = [y_hat[i] == y[i] for i in range(len(y_hat))]\n",
    "    correct = np.array([1 if i else 0 for i in correct])\n",
    "    accuracy = correct.sum()/len(y_hat)\n",
    "    print(f'Overall accuracy: {accuracy * 100.0}%')\n",
    "    return accuracy\n",
    "\n",
    "    \n",
    "# Get the list of files\n",
    "csv_files = glob.glob('../data/Day 2/BackBlaze/data_Q4_2020/*.csv')\n",
    "\n",
    "# Create the LazyFrame\n",
    "lazy_df = pl.scan_csv(csv_files)\n",
    "\n",
    "# Polars doesn't have a simple way to track categories, so we'll collect them and\n",
    "# make some dictionaries for lookups.\n",
    "models_df = lazy_df.select(pl.col('model')).cast(pl.Categorical).unique().collect(engine='streaming')\n",
    "model_to_number = {model:index for index,model in enumerate(models_df['model'])}\n",
    "number_to_model = {index:model for index,model in enumerate(models_df['model'])}\n",
    "\n",
    "# Begin the query plan by excluding some features\n",
    "# Manually reordering the columns so that the labels are first, making them easy to slice off later.\n",
    "# This also makes the correlation matrix easier to work with... more on that soon.\n",
    "lazy_df = lazy_df.select(pl.col('failure'), pl.col('model'), pl.col('^smart.*raw$'))\n",
    "\n",
    "# Make the model categorical and then cast to an integer. Remove nulls\n",
    "lazy_df = lazy_df.with_columns(pl.col(\"model\").cast(pl.Categorical).cast(pl.UInt32))\n",
    "lazy_df = lazy_df.with_columns(pl.all().cast(pl.Int64))\n",
    "lazy_df = lazy_df.fill_null(-1)\n",
    "\n",
    "# Add an index column to each row in a temporary LazyFrame named df_with_index\n",
    "df_with_index = lazy_df.with_row_index(name=\"index\")\n",
    "\n",
    "# Collect 10 failed and ten not failed drives into two different materialized dataframes using .limit(10).\n",
    "failed_df = df_with_index.filter(pl.col('failure') == 1).collect(engine='streaming').limit(10)\n",
    "not_failed_df = df_with_index.filter(pl.col('failure') == 0).collect(engine='streaming').limit(10)\n",
    "\n",
    "# Concatenate the two materialized dataframes into a dataframe named testing_df\n",
    "test_df = pl.concat([failed_df, not_failed_df])\n",
    "\n",
    "# Grab the indices of the values in the original data from this new dataframe\n",
    "test_indices = test_df['index'].to_numpy()\n",
    "\n",
    "# Create a new LazyFrame that adds to the current query plan, filtering out the test rows\n",
    "train_df = df_with_index.filter(~pl.col(\"index\").is_in(test_indices))\n",
    "\n",
    "# Drop the 'index' column from the test_df. Add dropping the 'index' column to the train_df query plan\n",
    "test_df = test_df.drop('index')\n",
    "train_df = train_df.drop('index')\n",
    "\n",
    "# Only perform the correlation if we have the memory for it\n",
    "import psutil\n",
    "\n",
    "memory = psutil.virtual_memory()\n",
    "gigs_total = memory.total // 1024**3\n",
    "\n",
    "# If the system has more than 16 gigabytes available, we can likely load the entire dataset.\n",
    "# We manually set the columns rather than doing it dynamically to save RAM and time.\n",
    "if gigs_total > 16:\n",
    "    print(f'Since your system has {gigs_total:,} gigabytes total, we will calculate the correlation matrix live.')\n",
    "    corr = lazy_df.collect(engine='streaming').corr()\n",
    "    print(\"Generated correlation\")\n",
    "    features_to_preserve = (np.abs(corr[0].to_numpy()) > 0.005)[0]\n",
    "    features_to_preserve[1] = True # preserve drive model\n",
    "    # Use `collect_schema()` to perform a lazy scan rather than materializing everything\n",
    "    relevant_columns = [col for i,col in enumerate(lazy_df.collect_schema().names()) if features_to_preserve[i]]\n",
    "else:\n",
    "    print(\"While your system meets the course requirements, this dataset is still too large. Preselecting relevant features...\")\n",
    "    relevant_columns = ['failure', 'model', 'smart_5_raw', 'smart_184_raw', 'smart_187_raw', 'smart_197_raw', 'smart_198_raw']\n",
    "print(f'Relevant features: {relevant_columns}')\n",
    "\n",
    "# Create the query plan for the training and testing dataframes\n",
    "train_df = train_df.select(relevant_columns)\n",
    "test_df = test_df.select(relevant_columns)\n",
    "\n",
    "# Load the dataframes and report on memory usage\n",
    "y_train = train_df.select('failure').collect(engine='streaming').to_numpy()\n",
    "x_train = train_df.drop('failure').collect(engine='streaming').to_numpy()\n",
    "x_test = test_df.drop('failure').to_numpy()\n",
    "y_test = test_df.select('failure').to_numpy()\n",
    "print(f'x_train is {x_train.nbytes:,} bytes in memory, y_train is {y_train.nbytes:,} bytes in memory.')\n",
    "print(f'Shape: x_train = {x_train.shape}, y_train = {y_train.shape}, x_test = {x_test.shape}, y_test = {y_test.shape}')\n",
    "\n",
    "# Reshape our label arrays:\n",
    "y_train = y_train.reshape(-1)\n",
    "y_test = y_test.reshape(-1)\n",
    "\n",
    "# Let's also generate synthetic data separately. This time we will try to use all of it!\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "smote = SMOTE()\n",
    "x_smote, y_smote = smote.fit_resample(x_train, y_train)\n",
    "np.bincount(y_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gbc = GradientBoostingClassifier(n_estimators=200, subsample=1.0, \n",
    "                                 max_features='sqrt', verbose=1, learning_rate=0.003)\n",
    "gbc.fit(x_smote, y_smote)\n",
    "predictions = gbc.predict(x_test)\n",
    "accuracy(y_test, predictions)\n",
    "\n",
    "preprocess_data = FunctionTransformer(preprocess_data)\n",
    "pipeline = pipeline.Pipeline(steps = [\n",
    "    ('preprocess dataframe', preprocess_data),\n",
    "    ('predictive model', gbc) ])\n",
    "\n",
    "#joblib.dump(pipeline, 'filename.joblib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explanation\n",
    "\n",
    "The code above replicates the final solution from the RandomForest/GBC lab in section 3. The only addition to the data loading is code that extracts and builds a dictionary mapping of the drive models to numbers based on the categorical processing in Polars. Unfortunately, there isn't a very clean way to get Polars to do this directly, so this was just the most expedient solution. We generate the dictionary and then generate the data. The categories will be identical since we are building them from the same dataset.\n",
    "\n",
    "After the pipeline is created, we save it with `joblib`. This has already been done. Please note that, if you are using a `FunctionTransformer` to handle things like preprocessing features (which we need to do here... we need to map the model name to a number and extract only the relevant features), you really need to have that function defined in a separate file that you are importing from. If you don't do this, pickle gets upset and doesn't know how to serialize it properly. The `preprocess_data()` function is in `pipeutils.py`.\n",
    "\n",
    "You must also import this function (as done below) to load the pipeline later, but as you can see the pipeline does make the code very clean. It's also very easy to take this and use it with FastAPI or some other platform to serve models like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To truly test this, you should kernel-restart and clear and then execute from here.\n",
    "# Notice that when saving a pipeline that contains a FunctionTransformer, you really\n",
    "# need to have that function (or class) imported from somewhere or you will end up\n",
    "# fighting very strange errors about objects being redefined. It is also critical that\n",
    "# your \"environment\" is the same, which in this case means we need to import that function\n",
    "# here so that it is defined. Notice that we do not need to import GBC or anything else.\n",
    "\n",
    "from sklearn import pipeline\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "import joblib\n",
    "import pandas as pd\n",
    "from pipeutils import preprocess_data\n",
    "\n",
    "model = joblib.load('GBC_backblaze_95.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/venv/lib/python3.11/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but GradientBoostingClassifier was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = pd.read_csv('drives.csv')\n",
    "model.predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
